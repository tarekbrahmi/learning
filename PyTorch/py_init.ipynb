{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "30a1c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99d162f",
   "metadata": {},
   "source": [
    "# 1 pytorch intro (tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddec92f",
   "metadata": {},
   "source": [
    "Everything in pytorch is based on Tensor operations.\n",
    "\n",
    "A tensor can have different dimensions ,so it can be 1d, 2d, or even 3d and higher\n",
    "\n",
    "scalar, vector, matrix, tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d3d8e0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.8952e-37, 0.0000e+00, 5.8037e-37])\n",
      "tensor([[6.9902e-37, 0.0000e+00, 6.1131e-37],\n",
      "        [0.0000e+00, 1.1210e-43, 0.0000e+00]])\n",
      "tensor([[[6.1478e-37, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 1.4013e-45, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 1.4013e-45],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3) # vector of 3 elements\n",
    "print(x)\n",
    "x = torch.empty(2,3) # matrix 2 list of 3 elements\n",
    "print(x)\n",
    "x = torch.empty(2,2,3) # 2 list that containe 2 list of 3 elements\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ded9ef16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8315, 0.4810, 0.4465],\n",
      "        [0.2115, 0.6844, 0.9171],\n",
      "        [0.5554, 0.9913, 0.3301],\n",
      "        [0.1091, 0.7622, 0.0841],\n",
      "        [0.1916, 0.6878, 0.4280]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 3) # 5 list of 3 elements(random)\n",
    "print(x)\n",
    "torch.zeros(5,3) # 5 list of 3 elements(0)\n",
    "torch.ones(5,3) # 5 list of 3 elements(1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "19db1fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(),x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "de118952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "# specify types, float32 default\n",
    "print(torch.zeros(5, 3, dtype=torch.float16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "63e6009c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 3.0000])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([5.5, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b3dfa9",
   "metadata": {},
   "source": [
    "# requires_grad argument\n",
    "This will tell pytorch that it will need to calculate the gradients for this tensor\n",
    "i.e. this is a variable in your model that you want to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "fcc604c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([5.5, 3], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1cc6ed",
   "metadata": {},
   "source": [
    "# operations with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "856643c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.ones(2,2)\n",
    "y=torch.ones(2,2)\n",
    "#addition each element x[i,j] + y[i,j]\n",
    "z=y.add_(x)\n",
    "#sub each element x[i,j] - y[i,j]\n",
    "z=y.sub(x)\n",
    "#mul each element x[i,j] * y[i,j]\n",
    "z=y.mul(x)\n",
    "#div each element x[i,j] / y[i,j]\n",
    "z=y.div(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "18f7189e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2.],\n",
       "        [2., 2.]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6858d3",
   "metadata": {},
   "source": [
    "# Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "00be1ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8755, 0.0850, 0.9109],\n",
      "        [0.4002, 0.9927, 0.1998],\n",
      "        [0.0863, 0.2828, 0.6171],\n",
      "        [0.6389, 0.7793, 0.7772],\n",
      "        [0.8153, 0.1452, 0.4304]])\n",
      "tensor([0.8755, 0.4002, 0.0863, 0.6389, 0.8153])\n",
      "tensor([0.4002, 0.9927, 0.1998])\n",
      "tensor([0.4002, 0.9927])\n",
      "tensor(0.9927)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "print(x[:, 0]) # all rows, column 0\n",
    "print(x[1, :]) # row 1, all columns\n",
    "print(x[1, :2]) # row 1, the first tow columns\n",
    "print(x[1,1]) # element at 1, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad64e3",
   "metadata": {},
   "source": [
    "# Reshape with torch.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "808e1a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1098, -0.3228,  0.2463, -0.1777],\n",
      "        [ 1.2212, -0.3734,  1.3100, -0.4451],\n",
      "        [-1.5143, -1.2796, -0.2747, -0.6193],\n",
      "        [-0.8648, -0.5364, -0.2694, -1.6441]]) torch.Size([4, 4])\n",
      "tensor([-0.1098, -0.3228,  0.2463, -0.1777,  1.2212, -0.3734,  1.3100, -0.4451,\n",
      "        -1.5143, -1.2796, -0.2747, -0.6193, -0.8648, -0.5364, -0.2694, -1.6441]) torch.Size([16])\n",
      "tensor([[-0.1098, -0.3228,  0.2463, -0.1777,  1.2212, -0.3734,  1.3100, -0.4451],\n",
      "        [-1.5143, -1.2796, -0.2747, -0.6193, -0.8648, -0.5364, -0.2694, -1.6441]]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8) # if -1 it pytorch will automatically determine the necessary size\n",
    "print(x,x.size())\n",
    "print(y,y.size())\n",
    "print(z,z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb51d5",
   "metadata": {},
   "source": [
    "# Converting a Torch Tensor to a NumPy array and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e8f90c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [1. 1.]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a=torch.ones(2,2)\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "print(type(b))\n",
    "# If the Tensor is on the CPU (not the GPU),\n",
    "# both objects will share the same memory location, so changing one\n",
    "# will also change the other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7570dc50",
   "metadata": {},
   "source": [
    "# numpy to torch with .from_numpy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8ade0b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a97ac5b",
   "metadata": {},
   "source": [
    "# 2 auto grad and using optemizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d0fcd",
   "metadata": {},
   "source": [
    "The autograd package provides automatic differentiation for all operations on Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c6cdb983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], requires_grad=True)\n",
      "tensor([[2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x7f7a30507610>\n"
     ]
    }
   ],
   "source": [
    "# requires_grad = True -> tracks all operations on the tensor (x). \n",
    "x = torch.zeros(3,5, requires_grad=True)\n",
    "y =  x +2\n",
    "\n",
    "# y was created as a result of an operation, so it has a grad_fn attribute.\n",
    "# grad_fn: references a Function that has created the Tensor\n",
    "print(x) # created by the user -> grad_fn is None\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f24f4",
   "metadata": {},
   "source": [
    "Let's compute the gradients with backpropagation\n",
    "When we finish our computation we can call .backward() and have all the gradients computed automatically.\n",
    "The gradient for this tensor will be accumulated into .grad attribute.\n",
    "It is the partial derivate of the function w.r.t. the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c3dd83c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4., 4.]], grad_fn=<MulBackward0>)\n",
      "tensor(4., grad_fn=<MeanBackward0>)\n",
      "tensor([[0.5333, 0.5333, 0.5333, 0.5333, 0.5333],\n",
      "        [0.5333, 0.5333, 0.5333, 0.5333, 0.5333],\n",
      "        [0.5333, 0.5333, 0.5333, 0.5333, 0.5333]])\n"
     ]
    }
   ],
   "source": [
    "z=y*y\n",
    "print(z)\n",
    "z=z.mean()\n",
    "print(z)\n",
    "\n",
    "z.backward()\n",
    "print(x.grad) # dz/dx\n",
    "# Generally speaking, torch.autograd is an engine for computing vector-Jacobian product\n",
    "# It computes partial derivates while applying the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985dd6b9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f5304ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8., 8., 8.], grad_fn=<MulBackward0>)\n",
      "torch.Size([3])\n",
      "tensor([8.0000e-01, 8.0000e+00, 8.0000e-04])\n"
     ]
    }
   ],
   "source": [
    "# Model with non-scalar output:\n",
    "x = torch.ones(3, requires_grad=True)\n",
    "y = x * 2\n",
    "for _ in range(2):\n",
    "    y = y * 2\n",
    "print(y)\n",
    "print(y.shape)\n",
    "\n",
    "# If a Tensor is non-scalar (more than 1 elements), we need to specify arguments for backward() \n",
    "# specify a gradient argument that is a tensor of matching shape.\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float32)\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fa2950b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "True\n",
      "<SumBackward0 object at 0x7f7a2fd17280>\n"
     ]
    }
   ],
   "source": [
    "# Stop a tensor from tracking history\n",
    "# For example during our training loop when we want to update our weights\n",
    "# then this update operation should not be part of the gradient computation\n",
    "a = torch.randn(2, 2)\n",
    "\n",
    "print(a.requires_grad)\n",
    "b = ((a * 3) / (a - 1))\n",
    "print(b.grad_fn)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "36cab39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([1., 1., 1.], requires_grad=True)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# get a new Tensor with the same content but no gradient computation:\n",
    "# wrap in 'with torch.no_grad():'\n",
    "a = torch.randn(2, 2, requires_grad=True)\n",
    "print(a.requires_grad)\n",
    "with torch.no_grad():\n",
    "    print(x)\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b9e5fe",
   "metadata": {},
   "source": [
    "# backward() accumulates the gradient for this tensor into .grad attribute.\n",
    "# !!! We need to be careful during optimization !!!\n",
    "# Use .zero_() to empty the gradients before a new optimization step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bd5ada21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1.], requires_grad=True)\n",
      "tensor([0.7000, 0.7000, 0.7000, 0.7000], requires_grad=True)\n",
      "tensor([0.4000, 0.4000, 0.4000, 0.4000], requires_grad=True)\n",
      "tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)\n",
      "tensor(4.8000, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    # just a dummy example\n",
    "    model_output = (weights*3).sum()\n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights)\n",
    "\n",
    "    # optimize model, i.e. adjust weights...\n",
    "    with torch.no_grad():\n",
    "        weights -= 0.1 * weights.grad\n",
    "\n",
    "    # this is important! It affects the final weights & output\n",
    "    weights.grad.zero_()\n",
    "\n",
    "print(weights)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8488eaca",
   "metadata": {},
   "source": [
    "# 3 backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e539d6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(1., grad_fn=<PowBackward0>)\n",
      "w.grad tensor(-2.)\n",
      "w now:  tensor(1.0200, requires_grad=True)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c53448a0",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "830e95cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 7: w = 1.997, loss = 0.00050331\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 11: w = 2.000, loss = 0.00000033\n",
      "epoch 13: w = 2.000, loss = 0.00000001\n",
      "epoch 15: w = 2.000, loss = 0.00000000\n",
      "epoch 17: w = 2.000, loss = 0.00000000\n",
      "epoch 19: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# gradientdescent manually\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "Y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model output\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "# J = MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2x(w*x - y)\n",
    "def gradient(x, y, y_pred):\n",
    "    return np.dot(2*x, y_pred - y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    # calculate gradients\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "\n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "     \n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "da54726c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 11: w = 1.665, loss = 1.16278565\n",
      "epoch 21: w = 1.934, loss = 0.04506890\n",
      "epoch 31: w = 1.987, loss = 0.00174685\n",
      "epoch 41: w = 1.997, loss = 0.00006770\n",
      "epoch 51: w = 1.999, loss = 0.00000262\n",
      "epoch 61: w = 2.000, loss = 0.00000010\n",
      "epoch 71: w = 2.000, loss = 0.00000000\n",
      "epoch 81: w = 2.000, loss = 0.00000000\n",
      "epoch 91: w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# gradientdescent auto\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "\n",
    "# here : f = 2 * x\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model output\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # predict = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_pred)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    #w.data = w.data - learning_rate * w.grad\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "    \n",
    "    # zero the gradients after updating\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {l.item():.8f}')\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5).item():.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae9116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
